{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESG-related content extracted and saved to: ESG_Content_Extracted.txt\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Define ESG-related keywords\n",
    "esg_keywords = [\n",
    "    \"ESG\", \"Environmental\", \"Social\", \"Governance\",\n",
    "    \"Sustainability\", \"Climate\", \"Carbon\", \"Diversity\",\n",
    "    \"Equity\", \"Inclusion\", \"Social Impact\", \"Emissions\",\n",
    "    \"Water Stewardship\", \"Waste Reduction\", \"Renewable\",\n",
    "    \"Human Rights\", \"Corporate Responsibility\"\n",
    "]\n",
    "\n",
    "# Path to the annual report PDF\n",
    "pdf_path = \"2024-Annual-Report.pdf\"\n",
    "\n",
    "# Function to extract ESG-related text\n",
    "def extract_esg_content(pdf_path, keywords):\n",
    "    esg_content = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in text.lower():\n",
    "                        esg_content.append(text)\n",
    "                        break  # Avoid duplicate entries from the same page\n",
    "    return esg_content\n",
    "\n",
    "# Extract ESG content\n",
    "esg_data = extract_esg_content(pdf_path, esg_keywords)\n",
    "\n",
    "# Save extracted content to a text file\n",
    "output_path = \"ESG_Content_Extracted.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(\"\\n\\n\".join(esg_data))\n",
    "\n",
    "# Output file location\n",
    "print(f\"ESG-related content extracted and saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ESG-related content...\n",
      "Applying Named Entity Recognition (NER)...\n",
      "Extraction complete. Data saved to Extracted_ESG_Entities.csv\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Load the spaCy NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define ESG-related keywords\n",
    "esg_keywords = [\n",
    "    \"ESG\", \"Environmental\", \"Social\", \"Governance\",\n",
    "    \"Sustainability\", \"Climate\", \"Carbon\", \"Diversity\",\n",
    "    \"Equity\", \"Inclusion\", \"Social Impact\", \"Emissions\",\n",
    "    \"Water Stewardship\", \"Waste Reduction\", \"Renewable\",\n",
    "    \"Human Rights\", \"Corporate Responsibility\"\n",
    "]\n",
    "\n",
    "# Function to extract ESG-related text from the PDF\n",
    "def extract_esg_text(pdf_path, keywords):\n",
    "    esg_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in text.lower():\n",
    "                        esg_text.append(text)\n",
    "                        break  # Avoid duplicate entries from the same page\n",
    "    return \" \".join(esg_text)  # Combine all ESG-related text\n",
    "\n",
    "# Function to apply Named Entity Recognition (NER)\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = {\"Organization\": [], \"Location\": [], \"Regulation\": []}\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\":\n",
    "            entities[\"Organization\"].append(ent.text)\n",
    "        elif ent.label_ == \"GPE\":\n",
    "            entities[\"Location\"].append(ent.text)\n",
    "        elif ent.label_ == \"LAW\":\n",
    "            entities[\"Regulation\"].append(ent.text)\n",
    "\n",
    "    # Remove duplicates\n",
    "    for key in entities:\n",
    "        entities[key] = list(set(entities[key]))\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    pdf_path = \"2024-Annual-Report.pdf\"  # Change to your file path\n",
    "    print(\"Extracting ESG-related content...\")\n",
    "    esg_text = extract_esg_text(pdf_path, esg_keywords)\n",
    "\n",
    "    if not esg_text:\n",
    "        print(\"No ESG-related content found in the document.\")\n",
    "        return\n",
    "\n",
    "    print(\"Applying Named Entity Recognition (NER)...\")\n",
    "    extracted_entities = extract_entities(esg_text)\n",
    "\n",
    "    # Convert extracted data into a DataFrame\n",
    "    df = pd.DataFrame(dict([(k, pd.Series(v)) for k, v in extracted_entities.items()]))\n",
    "\n",
    "    # Save to CSV\n",
    "    output_csv = \"Extracted_ESG_Entities.csv\"\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Extraction complete. Data saved to {output_csv}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe11980166846b595fac50900a2da95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac07708b08448178bdbbfd938a95352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7107bc9d5d9d4731bfe5b9f4d01cbebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b534b2327b76486c8313a9ce67911cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8794ee24ae44218d268baf1c13bce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc4a371c49134b70b49cfffefd5311ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ESG-related content...\n",
      "\n",
      "Summarizing key ESG insights...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/pytorch_utils.py:338: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load NLP summarization model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Define ESG-related keywords\n",
    "esg_keywords = [\n",
    "    \"ESG\", \"Environmental\", \"Social\", \"Governance\",\n",
    "    \"Sustainability\", \"Climate\", \"Carbon\", \"Diversity\",\n",
    "    \"Equity\", \"Inclusion\", \"Social Impact\", \"Emissions\",\n",
    "    \"Water Stewardship\", \"Waste Reduction\", \"Renewable\",\n",
    "    \"Human Rights\", \"Corporate Responsibility\"\n",
    "]\n",
    "\n",
    "# Extract ESG-related text from the PDF\n",
    "def extract_esg_text(pdf_path, keywords):\n",
    "    esg_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in text.lower():\n",
    "                        esg_text.append(text)\n",
    "                        break  # Avoid duplicate entries\n",
    "    return \" \".join(esg_text)  # Combine all ESG-related text\n",
    "\n",
    "# Summarize extracted ESG content with precise control\n",
    "def summarize_text(text):\n",
    "    if len(text.split()) < 50:  # If the content is too short, return as-is\n",
    "        return text\n",
    "\n",
    "    wrapped_text = textwrap.wrap(text, width=1024)  # Avoid input size issues\n",
    "    summary = []\n",
    "\n",
    "    for chunk in wrapped_text:\n",
    "        # Adjust max_length dynamically based on input size\n",
    "        words = len(chunk.split())\n",
    "        max_len = min(words, 150)  # Limit summary to max 150 words\n",
    "        min_len = max(50, max_len // 2)  # Ensure meaningful summary\n",
    "        \n",
    "        result = summarizer(chunk, max_length=max_len, min_length=min_len, do_sample=False)\n",
    "        summary.append(result[0]['summary_text'])\n",
    "\n",
    "    return \"\\n\".join(summary)\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    pdf_path = \"2024-Annual-Report.pdf\"  # Update with your actual file path\n",
    "    print(\"\\nExtracting ESG-related content...\\n\")\n",
    "    esg_text = extract_esg_text(pdf_path, esg_keywords)\n",
    "\n",
    "    if not esg_text:\n",
    "        print(\"No ESG-related content found.\")\n",
    "        return\n",
    "\n",
    "    print(\"Summarizing key ESG insights...\\n\")\n",
    "    summary = summarize_text(esg_text)\n",
    "\n",
    "    # Save summary to a text file\n",
    "    output_txt = \"ESG_Summary.txt\"\n",
    "    with open(output_txt, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(summary)\n",
    "\n",
    "    # Print preview of output\n",
    "    print(\"Summary Preview:\\n\")\n",
    "    print(summary[:1000])  # Print first 1000 characters for preview\n",
    "    print(f\"\\nSummary saved to: {output_txt}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from transformers import pipeline\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load NLP models\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Define ESG-related keywords\n",
    "esg_keywords = [\n",
    "    \"ESG\", \"Environmental\", \"Social\", \"Governance\",\n",
    "    \"Sustainability\", \"Climate\", \"Carbon\", \"Diversity\",\n",
    "    \"Equity\", \"Inclusion\", \"Social Impact\", \"Emissions\",\n",
    "    \"Water Stewardship\", \"Waste Reduction\", \"Renewable\",\n",
    "    \"Human Rights\", \"Corporate Responsibility\"\n",
    "]\n",
    "\n",
    "# Extract ESG-related text from the PDF\n",
    "def extract_esg_text(pdf_path, keywords):\n",
    "    esg_text = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                for keyword in keywords:\n",
    "                    if keyword.lower() in text.lower():\n",
    "                        esg_text.append(text)\n",
    "                        break\n",
    "    return \" \".join(esg_text)  # Combine ESG text\n",
    "\n",
    "# Count occurrences of ESG-related terms\n",
    "def count_esg_terms(text):\n",
    "    words = text.split()\n",
    "    word_counts = Counter(word.lower() for word in words if word.lower() in esg_keywords)\n",
    "    return word_counts.most_common(5)  # Top 5 ESG terms\n",
    "\n",
    "# Extract named entities\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entity_counts = Counter()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"ORG\", \"GPE\", \"LAW\"]:\n",
    "            entity_counts[ent.text] += 1\n",
    "    \n",
    "    return entity_counts.most_common(5)  # Top 5 entities\n",
    "\n",
    "# Perform sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment_score = blob.sentiment.polarity\n",
    "    \n",
    "    if sentiment_score > 0.1:\n",
    "        return \"Positive Sentiment\"\n",
    "    elif sentiment_score < -0.1:\n",
    "        return \"Negative Sentiment\"\n",
    "    else:\n",
    "        return \"Neutral Sentiment\"\n",
    "\n",
    "# Summarize ESG insights\n",
    "def summarize_esg(text):\n",
    "    if len(text.split()) < 50:\n",
    "        return text  # Return as-is if too short\n",
    "\n",
    "    summary = summarizer(text, max_length=150, min_length=50, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "# Generate structured output in Markdown format\n",
    "def generate_output(pdf_path):\n",
    "    print(\"Extracting ESG-related content...\\n\")\n",
    "    esg_text = extract_esg_text(pdf_path, esg_keywords)\n",
    "\n",
    "    if not esg_text:\n",
    "        print(\"No ESG-related content found.\")\n",
    "        return\n",
    "\n",
    "    # Compute results\n",
    "    esg_terms = count_esg_terms(esg_text)\n",
    "    named_entities = extract_entities(esg_text)\n",
    "    sentiment = analyze_sentiment(esg_text)\n",
    "    esg_summary = summarize_esg(esg_text)\n",
    "\n",
    "     # Format output properly for Markdown\n",
    "    output = f\"\"\"\n",
    "### 1. Most Common ESG Terms Found\n",
    "```diff\n",
    "{\"\".join(f\"- {term.capitalize()}: {count} occurrences\\n\" for term, count in esg_terms)}\"\"\"\n",
    "    {\"\".join(f\"- {entity} ({count} mentions)\\n\" for entity, count in named_entities)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
